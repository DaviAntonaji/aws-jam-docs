# ğŸ® Mix Hot and Cold - Redshift Streaming & Data Sharing

## ğŸ“‹ VisÃ£o Geral

Este desafio aborda **streaming de dados em tempo real** com Amazon Kinesis Data Streams, **ingestÃ£o para Amazon Redshift**, processamento de **dados semiestruturados** com tipo SUPER, e **compartilhamento de dados** entre clusters Redshift usando o recurso de **Data Sharing**.

### ğŸ¯ CenÃ¡rio

VocÃª trabalharÃ¡ com dados de **sessÃµes de jogos** (gaming) transmitidos em tempo real via Kinesis Data Streams para um Data Warehouse Redshift. VocÃª tambÃ©m juntarÃ¡ esses dados com dados de referÃªncia de jogadores usando o recurso de **Redshift Data Sharing**, permitindo consultas analÃ­ticas combinando dados "quentes" (streaming) e "frios" (referÃªncia).

> **ğŸ’¡ Nome do Desafio:** "Mix Hot and Cold" refere-se Ã  mistura de dados **quentes** (hot = streaming em tempo real) com dados **frios** (cold = dados de referÃªncia/histÃ³ricos)

---

## âš ï¸ Aviso Importante - Problema na Task 4

> **âŒ TASK 4 TEM PROBLEMA DE PERMISSÃ•ES**
> 
> **Problema:** A Task 4 requer criaÃ§Ã£o de Datashare no Redshift, mas o usuÃ¡rio do lab nÃ£o possui permissÃµes adequadas para criar datashares entre clusters.
>
> **Status:** 
> - Tasks 1-3: âœ… Funcionam normalmente
> - Task 4: âš ï¸ Parcialmente bloqueada (conceitos podem ser aprendidos, mas nÃ£o executados)
>
> **RecomendaÃ§Ã£o:** Complete Tasks 1-3 normalmente. Use Task 4 para aprendizado teÃ³rico de Data Sharing.

---

## ğŸ“ O Que VocÃª Vai Aprender

- âœ… Gerar dados de streaming com **Kinesis Data Generator**
- âœ… Criar e gerenciar **Kinesis Data Streams**
- âœ… IngestÃ£o de streaming em tempo real para **Amazon Redshift**
- âœ… Trabalhar com **External Schemas** (Kinesis no Redshift)
- âœ… Criar e usar **Materialized Views** com auto-refresh
- âœ… Processar dados semiestruturados com tipo **SUPER**
- âœ… Usar **PartiQL** para consultar dados JSON
- âœ… TÃ©cnicas de **UNNEST** e **UNPIVOT**
- âœ… Carregar dados do S3 para Redshift com COPY
- âœ… **Redshift Data Sharing** (teÃ³rico, devido ao problema)
- âœ… Consultas analÃ­ticas combinando mÃºltiplas fontes

## ğŸ› ï¸ ServiÃ§os AWS Utilizados

- **Amazon Kinesis Data Streams:** Streaming de dados em tempo real
- **Kinesis Data Generator:** GeraÃ§Ã£o de dados de teste
- **Amazon Redshift:** Data Warehouse
  - **gamesds cluster:** Para dados de jogos (streaming)
  - **playersds cluster:** Para dados de jogadores (referÃªncia)
- **Amazon S3:** Armazenamento de dados de referÃªncia
- **Redshift Query Editor V2:** Interface SQL
- **IAM Roles:** PermissÃµes para Redshift acessar Kinesis e S3

## ğŸ“¦ InventÃ¡rio

- âœ… **Kinesis Data Stream** (prÃ©-criado) para receber dados de jogos
- âœ… **Kinesis Data Generator** (URL fornecida) para simular streaming
- âœ… **Redshift Cluster "gamesds"** para dados de sessÃµes de jogos
- âœ… **Redshift Cluster "playersds"** para dados de jogadores
- âœ… **IAM Role "Redshiftgamesrole"** com permissÃµes adequadas
- âœ… **Dados de jogadores no S3** (champion_info.json, etc.)
- âœ… **Credenciais:** awsuser / Awsuser123

## ğŸ¯ Estrutura de Tarefas

### Task 1: Turn on the Game (15 pontos) ğŸ®

**Objetivo:** Gerar dados de sessÃ£o de jogos e transmitir para Kinesis Data Streams

**AÃ§Ãµes:**
- Usar Kinesis Data Generator para simular dados de jogos
- Configurar template JSON
- Iniciar streaming para KDS

**Resultado:** Nome do Kinesis Data Stream

---

### Task 2: Ingest Streaming Data (45 pontos) ğŸ“¥

**Objetivo:** Ingerir dados de streaming do KDS para Redshift e aplicar schema dinamicamente

**AÃ§Ãµes:**
- Criar External Schema apontando para Kinesis
- Criar Materialized View com auto-refresh
- Criar View para unnest dados semiestruturados (tipo SUPER)

**Resultado:** Tipo de dados do campo `payload` (SUPER)

---

### Task 3: Load Player Reference Data (45 pontos) ğŸ‘¥

**Objetivo:** Carregar dados de jogadores de diferentes esquemas JSON em uma Ãºnica tabela

**AÃ§Ãµes:**
- Criar tabela staging com tipo SUPER
- Carregar JSONs do S3 usando COPY
- Unnest e unpivot dados para tabela final

**Resultado:** Contagem total de jogadores registrados

---

### âš ï¸ Task 4: Share Data Between Clusters (45 pontos) ğŸ”—

**Objetivo:** Compartilhar dados de jogadores entre clusters usando Data Sharing

**ğŸ”´ PROBLEMA IDENTIFICADO:**
- Requer permissÃµes para criar Datashare
- UsuÃ¡rio do lab nÃ£o tem essas permissÃµes
- Conceitos podem ser aprendidos, mas nÃ£o executados

**AÃ§Ãµes (teÃ³ricas):**
- Criar datashare em playersds
- Adicionar tabela lol_players ao datashare
- Conceder acesso ao cluster gamesds
- Criar database local a partir do datashare
- Executar queries combinando dados compartilhados

**Resultado esperado:** Contagem de jogadores via datashare

---

## ğŸ“Š Arquitetura do Desafio

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Fluxo de Dados                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  1. Kinesis Data Generator                                 â”‚
â”‚      â”‚                                                      â”‚
â”‚      â”œâ”€â–º Gera JSON de sessÃµes de jogos                     â”‚
â”‚      â”‚                                                      â”‚
â”‚      â–¼                                                      â”‚
â”‚  2. Kinesis Data Stream (Hot Data - Streaming)             â”‚
â”‚      â”‚                                                      â”‚
â”‚      â”œâ”€â–º Real-time streaming                               â”‚
â”‚      â”‚                                                      â”‚
â”‚      â–¼                                                      â”‚
â”‚  3. Redshift "gamesds" Cluster                             â”‚
â”‚      â”‚                                                      â”‚
â”‚      â”œâ”€â–º External Schema (Kinesis)                         â”‚
â”‚      â”œâ”€â–º Materialized View (auto-refresh)                  â”‚
â”‚      â”œâ”€â–º View (unnested data) - tipo SUPER                 â”‚
â”‚      â”‚                                                      â”‚
â”‚      â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”             â”‚
â”‚      â”‚  â”‚  4. S3 (Cold Data - Reference)     â”‚             â”‚
â”‚      â”‚  â”‚     â”‚                               â”‚             â”‚
â”‚      â”‚  â”‚     â”œâ”€â–º champion_info.json         â”‚             â”‚
â”‚      â”‚  â”‚     â”œâ”€â–º champion_info_2.json       â”‚             â”‚
â”‚      â”‚  â”‚     â””â”€â–º summoner_spell_info.json   â”‚             â”‚
â”‚      â”‚  â”‚                                     â”‚             â”‚
â”‚      â”‚  â”‚  â–¼                                  â”‚             â”‚
â”‚      â”‚  â”‚  5. Redshift "playersds" Cluster   â”‚             â”‚
â”‚      â”‚  â”‚     â”‚                               â”‚             â”‚
â”‚      â”‚  â”‚     â”œâ”€â–º COPY from S3               â”‚             â”‚
â”‚      â”‚  â”‚     â”œâ”€â–º Staging table (SUPER)      â”‚             â”‚
â”‚      â”‚  â”‚     â”œâ”€â–º UNNEST / UNPIVOT           â”‚             â”‚
â”‚      â”‚  â”‚     â””â”€â–º lol_players table          â”‚             â”‚
â”‚      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€             â”‚
â”‚      â”‚               â”‚                                      â”‚
â”‚      â”‚               â”‚  6. Data Sharing                     â”‚
â”‚      â”‚               â”‚     (âš ï¸ PROBLEMA AQUI)               â”‚
â”‚      â”‚               â”‚                                      â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â–º 7. Analytic Queries          â”‚
â”‚                               Combining Hot + Cold          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Conceitos-Chave

### 1. Tipo SUPER no Redshift

**O que Ã©:**
Tipo de dados que suporta armazenamento e consulta de dados **semiestruturados** (JSON, Parquet, Avro) sem necessidade de schema fixo.

**BenefÃ­cios:**
- Schema-on-read (aplica schema na leitura, nÃ£o na escrita)
- Suporta dados aninhados
- Permite consultas PartiQL
- FlexÃ­vel para mÃºltiplos schemas

**Exemplo:**
```sql
CREATE TABLE staging (
  id INT,
  data SUPER  -- Pode conter qualquer estrutura JSON
);
```

### 2. Streaming Ingestion no Redshift

**External Schema para Kinesis:**
```sql
CREATE EXTERNAL SCHEMA kinesis_schema
FROM KINESIS
IAM_ROLE 'arn:aws:iam::...';
```

**Materialized View com Auto-Refresh:**
```sql
CREATE MATERIALIZED VIEW mv_streaming
  sortkey(refresh_time)
  distkey(sequence_number)
  auto refresh yes AS
SELECT
  approximate_arrival_timestamp,
  json_parse(kinesis_data) as payload  -- Tipo SUPER
FROM kinesis_schema.stream_name;
```

**CaracterÃ­sticas:**
- Auto-refresh a cada ~10 segundos
- Dados chegam quase em tempo real
- Usa `json_parse()` para converter para SUPER

### 3. UNNEST e UNPIVOT

**UNNEST:**
Expande arrays em linhas individuais.

**UNPIVOT:**
Transforma colunas em linhas, especialmente Ãºtil para JSON dinÃ¢mico.

**Exemplo:**
```sql
SELECT
  value.id::int,
  value.name::varchar
FROM staging_table,
UNPIVOT data AS value AT key;
```

### 4. Redshift Data Sharing

**O que Ã©:**
Permite compartilhar dados em **tempo real** entre clusters Redshift sem copiar ou mover dados.

**BenefÃ­cios:**
- Isolamento de workloads (produÃ§Ã£o vs anÃ¡lise)
- Sem duplicaÃ§Ã£o de dados
- Acesso instantÃ¢neo
- Controle granular de permissÃµes

**Processo (teÃ³rico):**
```sql
-- No cluster produtor (playersds):
CREATE DATASHARE player_share;
ALTER DATASHARE player_share ADD SCHEMA public;
ALTER DATASHARE player_share ADD TABLE public.lol_players;
GRANT USAGE ON DATASHARE player_share TO NAMESPACE 'consumer-namespace';

-- No cluster consumidor (gamesds):
CREATE DATABASE shared_db FROM DATASHARE player_share OF NAMESPACE 'producer-namespace';
SELECT * FROM shared_db.public.lol_players;
```

### 5. Case Sensitivity no Redshift

**Por que Ã© necessÃ¡rio:**
JSON frequentemente tem campos em CamelCase ou UPPERCASE.

**Como configurar:**
```sql
SET enable_case_sensitive_identifier TO true;
```

**Uso:**
```sql
SELECT
  (payload."gameId")::varchar,  -- Aspas duplas para case-sensitive
  (payload."creationTime")::numeric
FROM mv_streaming;
```

---

## ğŸ† CritÃ©rios de Sucesso

### Tasks 1-3 (Funcionam):
- [ ] GeraÃ§Ã£o de dados de streaming funcionando
- [ ] Kinesis Data Stream recebendo dados
- [ ] External Schema criado no Redshift
- [ ] Materialized View ingesting dados
- [ ] Tipo SUPER identificado e usado corretamente
- [ ] Dados unnested e parsed
- [ ] Dados de jogadores carregados do S3
- [ ] UNPIVOT aplicado corretamente
- [ ] Contagem de jogadores obtida

### Task 4 (Problema):
- [ ] CompreensÃ£o teÃ³rica de Data Sharing
- [ ] Entendimento de quando usar Data Sharing
- [ ] Conhecimento da sintaxe SQL de datashares
- âš ï¸ CriaÃ§Ã£o de datashare (bloqueada por permissÃµes)
- âš ï¸ Queries cross-cluster (nÃ£o pode ser testado)

---

## ğŸ“š Recursos Ãšteis

### DocumentaÃ§Ã£o AWS
- [Amazon Kinesis Data Streams](https://docs.aws.amazon.com/streams/latest/dev/)
- [Streaming Ingestion in Redshift](https://docs.aws.amazon.com/redshift/latest/dg/materialized-view-streaming-ingestion.html)
- [Redshift SUPER Data Type](https://docs.aws.amazon.com/redshift/latest/dg/r_SUPER_type.html)
- [Redshift Data Sharing](https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html)
- [PartiQL in Redshift](https://docs.aws.amazon.com/redshift/latest/dg/query-super.html)

### Blogs e Artigos
- [Sharing Redshift Data Securely](https://aws.amazon.com/blogs/big-data/sharing-amazon-redshift-data-securely-across-amazon-redshift-clusters-for-workload-isolation/)
- [SUPER Data Type Deep Dive](https://aws.amazon.com/blogs/big-data/query-and-visualize-nested-data-in-amazon-redshift-using-the-super-data-type/)
- [Streaming Ingestion Best Practices](https://aws.amazon.com/blogs/big-data/stream-data-into-amazon-redshift-for-near-real-time-analytics/)

### Ferramentas
- [Kinesis Data Generator](https://awslabs.github.io/amazon-kinesis-data-generator/)

---

## ğŸ¯ PontuaÃ§Ã£o Total

| Task | Pontos | Status |
|------|--------|--------|
| Task 1: Turn on the Game | 15 | âœ… Funciona |
| Task 2: Ingest Streaming Data | 45 | âœ… Funciona |
| Task 3: Load Player Data | 45 | âœ… Funciona |
| Task 4: Data Sharing | 45 | âš ï¸ Problema de permissÃµes |
| **TOTAL** | **150** | **105 possÃ­veis** |

---

## â±ï¸ Tempo Estimado

- **Task 1:** 10-15 minutos
- **Task 2:** 20-30 minutos (aguardar refresh de MV)
- **Task 3:** 20-30 minutos
- **Task 4:** 15-20 minutos (se funcionasse)
- **Total:** 65-95 minutos

---

## ğŸ¯ Dificuldade

**â­â­â­â˜†â˜† Medium**

Requer conhecimento de:
- SQL intermediÃ¡rio/avanÃ§ado
- Conceitos de streaming de dados
- Tipos de dados semiestruturados
- Redshift-specific features

---

## ğŸ’¡ Dicas Importantes

### Geral
1. âœ… **Mantenha duas abas** do Query Editor abertas (uma para cada cluster)
2. âœ… **Verifique o cluster selecionado** antes de executar queries
3. âœ… **Use DISTINCT id** ao contar jogadores (nÃ£o apenas COUNT(*))
4. âœ… **Aguarde ~10-20 segundos** para Materialized View atualizar

### Task 1
- Stream name aparece automaticamente no Data Generator
- Use partition key: `gameId` ou `{{random.number()}}`
- Records per second: 5-50 Ã© suficiente

### Task 2
- Execute `SET enable_case_sensitive_identifier TO true;` antes de criar views
- Use aspas duplas em campos JSON com maiÃºsculas
- Aguarde alguns segundos apÃ³s iniciar streaming para ver dados

### Task 3
- Confirme que estÃ¡ conectado ao cluster **playersds**
- Execute os 3 COPYs sequencialmente
- Use `COUNT(DISTINCT id)` para contar jogadores Ãºnicos

### Task 4
- âš ï¸ Se encontrar erro de permissÃµes, Ã© esperado
- Estude a sintaxe de Data Sharing para uso futuro
- Conceito Ã© importante mesmo sem poder executar

---

## ğŸš¨ Troubleshooting

### Problema: NÃ£o vejo dados na Materialized View

**Causas possÃ­veis:**
- Streaming nÃ£o foi iniciado no Data Generator
- Aguardou menos de 10 segundos
- External Schema aponta para stream incorreto

**SoluÃ§Ã£o:**
```sql
-- Verificar se hÃ¡ dados
SELECT COUNT(*) FROM mv_lol_game_session;

-- Ver refresh times
SELECT DISTINCT refresh_time 
FROM mv_lol_game_session 
ORDER BY refresh_time DESC;
```

### Problema: Erro ao fazer COPY do S3

**Causas possÃ­veis:**
- IAM Role ARN incorreto
- Conectado ao cluster errado
- Bucket/arquivo nÃ£o existe

**SoluÃ§Ã£o:**
- Copie ARN exato das Output Properties
- Verifique dropdown do cluster
- Use exatamente: `s3://redshift-demos/data/players/`

### Problema: SUPER data type nÃ£o reconhecido

**Causas possÃ­veis:**
- VersÃ£o antiga do Redshift (nÃ£o Ã© o caso no lab)
- Syntax error no SQL

**SoluÃ§Ã£o:**
```sql
-- Testar SUPER
CREATE TABLE test_super (id INT, data SUPER);
INSERT INTO test_super VALUES (1, JSON_PARSE('{"key":"value"}'));
SELECT * FROM test_super;
```

### Problema: Task 4 - Erro ao criar Datashare

**Causa:**
- PermissÃµes IAM faltantes no usuÃ¡rio do lab

**SoluÃ§Ã£o:**
- âŒ NÃ£o hÃ¡ workaround no lab atual
- âœ… Estude a documentaÃ§Ã£o de Data Sharing
- âœ… Use em conta prÃ³pria para praticar

---

## ğŸ­ AplicaÃ§Ã£o em ProduÃ§Ã£o

### Melhorias para Ambiente Real

**1. Alta Disponibilidade:**
- MÃºltiplos shards no Kinesis
- Redshift cluster multi-node
- Backup e disaster recovery

**2. Monitoramento:**
- CloudWatch metrics para Kinesis
- Redshift query monitoring
- Alerts para latÃªncia de streaming

**3. OtimizaÃ§Ã£o:**
- Tuning de distribution e sort keys
- Ajuste de intervalo de auto-refresh
- Particionamento de dados histÃ³ricos

**4. SeguranÃ§a:**
- Encryption at rest (S3, Redshift)
- Encryption in transit (Kinesis, Redshift)
- VPC Endpoints para trÃ¡fego privado
- Fine-grained access control

**5. Custo:**
- Right-sizing de clusters Redshift
- Kinesis on-demand vs provisioned
- S3 lifecycle policies
- Pause de clusters quando nÃ£o usados

---

## ğŸ“ PrÃ³ximos Passos

ApÃ³s completar este desafio:
1. **Pratique em conta prÃ³pria** para experiÃªncia completa com Data Sharing
2. **Explore** outras fontes de streaming (DynamoDB Streams, MSK)
3. **Aprenda** Redshift Spectrum para consultas diretas ao S3
4. **Estude** RA3 nodes e managed storage no Redshift
5. **Implemente** pipelines de dados end-to-end

---

**ğŸ‰ Boa sorte com o desafio!**

> **ğŸ’­ ReflexÃ£o:** Este desafio demonstra como combinar dados em tempo real (hot) com dados de referÃªncia (cold) em um Data Warehouse moderno, uma arquitetura comum em cenÃ¡rios de analytics de IoT, gaming, e e-commerce.
